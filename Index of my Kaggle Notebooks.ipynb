{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def nbs_to_df(nbs):\n",
    "    parsed_notebooks = [parse(a_notebook.split()) for a_notebook in nbs]\n",
    "    df = pd.DataFrame(parsed_notebooks, columns=['title', 'slug', 'total_votes', 'date'])\n",
    "    df = df[df['total_votes'] > 0 ]\n",
    "    df['url'] = df.apply(to_url, axis=1)\n",
    "    df = df.sort_values(\"title\", ascending=True)\n",
    "    return df\n",
    "\n",
    "def parse(a_notebook):\n",
    "    slug = a_notebook[0]\n",
    "    total_votes = int(a_notebook[-1])\n",
    "    title = ' '.join(a_notebook[1:-6])\n",
    "    date = ' '.join(a_notebook[-3:-2])\n",
    "    return (title, slug, total_votes, date)\n",
    "\n",
    "def to_url(row, include_bullet=True, include_votes=True):\n",
    "    txt = f\"[{row.title}](http://kaggle.com/{row.slug})\"\n",
    "    if include_bullet:\n",
    "        txt = f\"* {txt}\"\n",
    "    if include_votes:\n",
    "        txt = f\"{txt} ({row.total_votes})\"\n",
    "    return txt\n",
    "\n",
    "\n",
    "def get_section(df, title, subtitle, url, matcher, sort_by):\n",
    "    asc = sort_by != 'total_votes'\n",
    "    if type(matcher) == str:\n",
    "        dfx = df[df['title'].str.contains(matcher)]\n",
    "    else:\n",
    "        dfx = df[df.title.apply(matcher)]\n",
    "    dfx = dfx.sort_values(sort_by, ascending=asc)\n",
    "    \n",
    "    txt = f\"## [{title}]({url})\\n\"\n",
    "    txt += f\"**{subtitle}**\\n\\n\"\n",
    "    \n",
    "    txt += '\\n'.join(dfx['url'])\n",
    "    return txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbs = !kaggle kernels list -m --page-size 100 --sort-by voteCount\n",
    "df = nbs_to_df(nbs[2:])\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# By Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_other_competitions(title):\n",
    "    for t in [ \"ğŸ¦ \", \"ğŸ‡®ğŸ‡³\", \"ğŸ \", \"ğŸ“–\", \"H&M\", \"ğŸ’²\", \"ğŸ¦\", \"â˜£ï¸\"]:\n",
    "        if t in title:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "COMPETITIONS = [\n",
    "    (\"Sartorius - Cell Instance Segmentation\", \n",
    "     \"Detect single neuronal cells in microscopy images\", \n",
    "     \"https://www.kaggle.com/c/sartorius-cell-instance-segmentation\", \n",
    "     \"ğŸ¦ \",\n",
    "     'total_votes'),\n",
    "    (\"chaii - Hindi and Tamil Question Answering\", \n",
    "     \"Identify the answer to questions found in Indian language passages\", \n",
    "     \"https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering\", \n",
    "     \"ğŸ‡®ğŸ‡³\",\n",
    "     'title'),\n",
    "    \n",
    "    (\"Jigsaw Rate Severity of Toxic Comments\",\n",
    "    \"Rank relative ratings of toxicity between comments\",\n",
    "    \"https://www.kaggle.com/c/jigsaw-toxic-severity-rating\",\n",
    "     \"â˜£ï¸\",\n",
    "     'total_votes'),\n",
    "    (\"TensorFlow - Help Protect the Great Barrier Reef\",\n",
    "    \"Detect crown-of-thorns starfish in underwater image data\",\n",
    "    \"https://www.kaggle.com/c/tensorflow-great-barrier-reef\",\n",
    "     \"ğŸ \",\n",
    "     \"total_votes\"),\n",
    "    \n",
    "    \n",
    "    (\"Feedback Prize - Evaluating Student Writing\",\n",
    "    \"Analyze argumentative writing elements from students grade 6-12\",\n",
    "    \"https://www.kaggle.com/c/feedback-prize-2021\",\n",
    "     \"ğŸ“–\",\n",
    "     \"total_votes\"),\n",
    "     \n",
    "    \n",
    "    (\"H&M Personalized Fashion Recommendations\",\n",
    "    \"Provide product recommendations based on previous purchases\",\n",
    "    \"https://www.kaggle.com/c/h-and-m-personalized-fashion-recommendations\",\n",
    "     \"H&M\",\n",
    "     \"total_votes\"),\n",
    "    \n",
    "    (\"G-Research Crypto Forecasting\",\n",
    "    \"Use your ML expertise to predict real crypto market data\",\n",
    "    \"https://www.kaggle.com/c/g-research-crypto-forecasting\",\n",
    "    \"ğŸ’²\" ,\n",
    "     \"total_votes\"),\n",
    "    \n",
    "    (\"BirdCLEF 2022\",\n",
    "    \"Identify bird calls in soundscapes\",\n",
    "    \"https://www.kaggle.com/c/birdclef-2022\",\n",
    "    \"ğŸ¦\",\n",
    "     \"total_votes\"),\n",
    "    \n",
    "    (\"Other competitions\",\n",
    "     \"\",\n",
    "     \"\",\n",
    "     is_other_competitions,\n",
    "     \"title\")\n",
    "         \n",
    "]\n",
    "\n",
    "\n",
    "def by_competition():\n",
    "    txt = \"# Organized by competition\\n\\n\"\n",
    "    for title, subtitle, url, matching_str, sort_by in COMPETITIONS:\n",
    "        txt += get_section(df, title, subtitle, url, matching_str, sort_by)\n",
    "        txt += \"\\n\\n\"\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Highlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIGHLIGHTS = [\n",
    "    \n",
    "    (\"ğŸ¦  Sartorius - Starter Torch Mask R-CNN [LB=0.273]\",\n",
    "    \"\"\"A self-contained, simple, pure Torch Mask R-CNN implementation baseline model for the competition [Sartorius - Cell Instance Segmentation](https://www.kaggle.com/c/sartorius-cell-instance-segmentation).\n",
    "    This notebook has more than 600 forks and was a high-performing end-to-end simple solution for the problem at the very beginning of the competition.\"\"\"),\n",
    "    \n",
    "    (\"Jigsaw - Incredibly Simple Naive Bayes [0.768]\",\n",
    "    \"\"\"A very simple naive bayes model with a high accuracy. \n",
    "    The condensed code has less than 20 lines and a slight -but smart- modification of it (adapted from a notebook by Jeremy Howard) would have landed a Bronze medal: [â˜£ï¸ Jigsaw - Jeremy Howard's NB [0.79744 Private]](https://www.kaggle.com/julian3833/jigsaw-jeremy-howard-s-nb-0-79744-private).\"\"\"),\n",
    "    ('PyTorch- \"ShortFormer\" w/Chunks - Train [0.624]',\n",
    "    \"\"\"A baseline model for the competition [Feedback Prize - Evaluating Student Writing](https://www.kaggle.com/c/feedback-prize-2021). \n",
    "    It approaches the problems as a token classification problem (\"NER\"-like) and builds a RoBERTa base model with `max_length=512`. \n",
    "    In order to do so, it manages the chunking with stride of the texts with length greater than 512 (and the posterior merge). \n",
    "    A discussion about Longformers is presented as well.\"\"\"),    \n",
    "    #('PyTorch- \"ShortFormer\" w/Chunks - Infer [0.624]',\n",
    "    #\"The inference notebook for the previous model.\"),\n",
    "    (\"H&M - Implicit ALS model [0.014]\",\n",
    "    \"Implicit ALS base model for the competition [H&M Personalized Fashion Recommendations](https://www.kaggle.com/c/h-and-m-personalized-fashion-recommendations).\"),\n",
    "    (\"1 - Quick start: read csv and flatten json fields\",\n",
    "    \"\"\"A simple utility function that flattens json fields in a CSV when loaded with `pandas` for the competition [Google Analytics Customer Revenue Prediction](https://www.kaggle.com/c/ga-customer-revenue-prediction). \n",
    "    This problem appeared quite fast, so everyone was getting blocked right after loading the CSV. \n",
    "    That is why the notebook has almost 500 upvotes and more than 550 forks having just 10 lines of code. The competition had a terrible leak.\"\"\"),\n",
    "    (\"GPT-2 Large â€“774Mâ€“ w/Pytorch: Not that impressive\", \n",
    "     \"\"\"In this old notebook I applied the out-of-the-box GPT-2 models (`gpt2`, `gpt2-medium`, and `gpt2-large`) on the samples in the original blog post ([Better Language Models and Their Implications](https://openai.com/blog/better-language-models/)) using `huggingface`'s [pytorch-transformers](https://github.com/huggingface/pytorch-transformers) library, with a pretty simple code based on the library's [Quick Start](https://huggingface.co/pytorch-transformers/quickstart.html). \n",
    "The conclussion was that the results were good but not that impressive as the blogpost suggested.\"\"\")    \n",
    "]\n",
    "\n",
    "def is_highlight(row):\n",
    "    for h, _ in HIGHLIGHTS:\n",
    "        if h in row['title']:\n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def get_highlight_order(row):\n",
    "    for i, (h, _) in enumerate(HIGHLIGHTS):\n",
    "        if h in row['title']:\n",
    "            return i\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_subtitle(row):\n",
    "    for h, subtitle in HIGHLIGHTS:\n",
    "        if h in row['title']:\n",
    "            return subtitle\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "def highlights(df):\n",
    "    df_highlights = df.loc[df.apply(is_highlight, axis=1), ['title', 'url', 'total_votes']].copy()\n",
    "    df_highlights['order'] = df_highlights.apply(get_highlight_order, axis=1)\n",
    "    df_highlights['subtitle'] = df_highlights.apply(get_subtitle, axis=1)\n",
    "    df_highlights = df_highlights.sort_values(\"order\")\n",
    "    \n",
    "    txt = \"# Highlights\\n\\n\"\n",
    "    for _, row in df_highlights.iterrows():\n",
    "        txt += row.url.replace(\"*\", \"##\")\n",
    "        txt += f\"\\n{row.subtitle}\\n\\n\"\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Highlights\n",
      "\n",
      "## [ğŸ¦  Sartorius - Starter Torch Mask R-CNN [LB=0.273]](http://kaggle.com/julian3833/sartorius-starter-torch-mask-r-cnn-lb-0-273) (278)\n",
      "A self-contained, simple, pure Torch Mask R-CNN implementation baseline model for the competition [Sartorius - Cell Instance Segmentation](https://www.kaggle.com/c/sartorius-cell-instance-segmentation).\n",
      "    This notebook has more than 600 forks and was a high-performing end-to-end simple solution for the problem at the very beginning of the competition.\n",
      "\n",
      "## [â˜£ï¸ Jigsaw - Incredibly Simple Naive Bayes [0.768]](http://kaggle.com/julian3833/jigsaw-incredibly-simple-naive-bayes-0-768) (186)\n",
      "A very simple naive bayes model with a high accuracy. \n",
      "    The condensed code has less than 20 lines and a slight -but smart- modification of it (adapted from a notebook by Jeremy Howard) would have landed a Bronze medal: [â˜£ï¸ Jigsaw - Jeremy Howard's NB [0.79744 Private]](https://www.kaggle.com/julian3833/jigsaw-jeremy-howard-s-nb-0-79744-private).\n",
      "\n",
      "## [ğŸ“– PyTorch- \"ShortFormer\" w/Chunks - Train [0.624]](http://kaggle.com/julian3833/pytorch-shortformer-w-chunks-train-0-624) (82)\n",
      "A baseline model for the competition [Feedback Prize - Evaluating Student Writing](https://www.kaggle.com/c/feedback-prize-2021). \n",
      "    It approaches the problems as a token classification problem (\"NER\"-like) and builds a RoBERTa base model with `max_length=512`. \n",
      "    In order to do so, it manages the chunking with stride of the texts with length greater than 512 (and the posterior merge). \n",
      "    A discussion about Longformers is presented as well.\n",
      "\n",
      "## [H&M - Implicit ALS model [0.014]](http://kaggle.com/julian3833/h-m-implicit-als-model-0-014) (163)\n",
      "Implicit ALS base model for the competition [H&M Personalized Fashion Recommendations](https://www.kaggle.com/c/h-and-m-personalized-fashion-recommendations).\n",
      "\n",
      "## [1 - Quick start: read csv and flatten json fields](http://kaggle.com/julian3833/1-quick-start-read-csv-and-flatten-json-fields) (491)\n",
      "A simple utility function that flattens json fields in a CSV when loaded with `pandas` for the competition [Google Analytics Customer Revenue Prediction](https://www.kaggle.com/c/ga-customer-revenue-prediction). \n",
      "    This problem appeared quite fast, so everyone was getting blocked right after loading the CSV. \n",
      "    That is why the notebook has almost 500 upvotes and more than 550 forks having just 10 lines of code. The competition had a terrible leak.\n",
      "\n",
      "## [GPT-2 Large â€“774Mâ€“ w/Pytorch: Not that impressive](http://kaggle.com/julian3833/gpt-2-large-774m-w-pytorch-not-that-impressive) (14)\n",
      "In this old notebook I applied the out-of-the-box GPT-2 models (`gpt2`, `gpt2-medium`, and `gpt2-large`) on the samples in the original blog post ([Better Language Models and Their Implications](https://openai.com/blog/better-language-models/)) using `huggingface`'s [pytorch-transformers](https://github.com/huggingface/pytorch-transformers) library, with a pretty simple code based on the library's [Quick Start](https://huggingface.co/pytorch-transformers/quickstart.html). \n",
      "The conclussion was that the results were good but not that impressive as the blogpost suggested.\n",
      "\n",
      "\n",
      "---\n",
      "# Organized by competition\n",
      "\n",
      "## [Sartorius - Cell Instance Segmentation](https://www.kaggle.com/c/sartorius-cell-instance-segmentation)\n",
      "**Detect single neuronal cells in microscopy images**\n",
      "\n",
      "* [ğŸ¦  Sartorius - Starter Torch Mask R-CNN [LB=0.273]](http://kaggle.com/julian3833/sartorius-starter-torch-mask-r-cnn-lb-0-273) (278)\n",
      "* [ğŸ¦  Sartorius - Starter Baseline Torch U-net [>0.0]](http://kaggle.com/julian3833/sartorius-starter-baseline-torch-u-net-0-0) (174)\n",
      "* [ğŸ¦  Sartorius - Classifier + Mask R-CNN [LB=0.28]](http://kaggle.com/julian3833/sartorius-classifier-mask-r-cnn-lb-0-28) (115)\n",
      "* [ğŸ¦  Sartorius - Resnet34 Classifier](http://kaggle.com/julian3833/sartorius-resnet34-classifier) (43)\n",
      "* [ğŸ¦  Sartorius - 3-line overlap-removing function](http://kaggle.com/julian3833/sartorius-3-line-overlap-removing-function) (13)\n",
      "\n",
      "## [chaii - Hindi and Tamil Question Answering](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering)\n",
      "**Identify the answer to questions found in Indian language passages**\n",
      "\n",
      "* [1 - The competition [QA for QA noobs ğŸ‡®ğŸ‡³]](http://kaggle.com/julian3833/1-the-competition-qa-for-qa-noobs) (36)\n",
      "* [2 - The dataset [QA for QA noobs ğŸ‡®ğŸ‡³]](http://kaggle.com/julian3833/2-the-dataset-qa-for-qa-noobs) (20)\n",
      "* [3 - The metric (Jaccard) [QA for QA noobs ğŸ‡®ğŸ‡³]](http://kaggle.com/julian3833/3-the-metric-jaccard-qa-for-qa-noobs) (19)\n",
      "* [4 - Exploring Public Models [QA for QA noobs ğŸ‡®ğŸ‡³]](http://kaggle.com/julian3833/4-exploring-public-models-qa-for-qa-noobs) (30)\n",
      "* [5-ğŸ¥‡XLM-Roberta+Torch's extra data [LB:0.749] ğŸ‡®ğŸ‡³](http://kaggle.com/julian3833/5-xlm-roberta-torch-s-extra-data-lb-0-749) (60)\n",
      "* [6- ğŸ¤— Pre & post-processing [QA for QA noobs ğŸ‡®ğŸ‡³]](http://kaggle.com/julian3833/6-pre-post-processing-qa-for-qa-noobs) (29)\n",
      "* [7 - Public Models Revisited [QA for QA noobs ğŸ‡®ğŸ‡³]](http://kaggle.com/julian3833/7-public-models-revisited-qa-for-qa-noobs) (30)\n",
      "* [Quick and Dirty Transliteration Tables ğŸ‡®ğŸ‡³](http://kaggle.com/julian3833/quick-and-dirty-transliteration-tables) (13)\n",
      "\n",
      "## [Jigsaw Rate Severity of Toxic Comments](https://www.kaggle.com/c/jigsaw-toxic-severity-rating)\n",
      "**Rank relative ratings of toxicity between comments**\n",
      "\n",
      "* [â˜£ï¸ Jigsaw - Incredibly Simple Naive Bayes [0.768]](http://kaggle.com/julian3833/jigsaw-incredibly-simple-naive-bayes-0-768) (186)\n",
      "* [â˜£ï¸ Jigsaw - Early Ensemble [LB=0.836]](http://kaggle.com/julian3833/jigsaw-early-ensemble-lb-0-836) (103)\n",
      "* [â˜£ï¸ Jigsaw - ğŸ¤— HF hub out-of-the-box models](http://kaggle.com/julian3833/jigsaw-hf-hub-out-of-the-box-models) (31)\n",
      "* [â˜£ï¸ Jigsaw - Explore Previous Competitions Datasets](http://kaggle.com/julian3833/jigsaw-explore-previous-competitions-datasets) (9)\n",
      "* [â˜£ï¸ Jigsaw - Random rank [LB=0.498]](http://kaggle.com/julian3833/jigsaw-random-rank-lb-0-498) (9)\n",
      "* [â˜£ï¸ Jigsaw - Jeremy Howard's NB [0.79744 Private]](http://kaggle.com/julian3833/jigsaw-jeremy-howard-s-nb-0-79744-private) (7)\n",
      "* [â˜£ï¸ Jigsaw - New Ensemble [LB=0.853]](http://kaggle.com/julian3833/jigsaw-new-ensemble-lb-0-853) (1)\n",
      "\n",
      "## [TensorFlow - Help Protect the Great Barrier Reef](https://www.kaggle.com/c/tensorflow-great-barrier-reef)\n",
      "**Detect crown-of-thorns starfish in underwater image data**\n",
      "\n",
      "* [ğŸ  Reef - A CV strategy: subsequences!](http://kaggle.com/julian3833/reef-a-cv-strategy-subsequences) (221)\n",
      "* [ğŸ  Reef- Starter Torch FasterRCNN Train [LB=0.416]](http://kaggle.com/julian3833/reef-starter-torch-fasterrcnn-train-lb-0-416) (136)\n",
      "* [ğŸ  Reef- Starter Torch FasterRCNN Infer [LB=0.416]](http://kaggle.com/julian3833/reef-starter-torch-fasterrcnn-infer-lb-0-416) (90)\n",
      "* [ğŸ  DETR - Detection Transformer - Train [0.189]](http://kaggle.com/julian3833/detr-detection-transformer-train-0-189) (32)\n",
      "* [ğŸ  DETR - Detection Transformer - Infer [0.189]](http://kaggle.com/julian3833/detr-detection-transformer-infer-0-189) (27)\n",
      "* [ğŸ  Reef - Minimal EDA](http://kaggle.com/julian3833/reef-minimal-eda) (4)\n",
      "\n",
      "## [Feedback Prize - Evaluating Student Writing](https://www.kaggle.com/c/feedback-prize-2021)\n",
      "**Analyze argumentative writing elements from students grade 6-12**\n",
      "\n",
      "* [ğŸ“–Feedback- BaselineğŸ¤— Sentence Classifier [0.226]](http://kaggle.com/julian3833/feedback-baseline-sentence-classifier-0-226) (189)\n",
      "* [ğŸ“– PyTorch- \"ShortFormer\" w/Chunks - Train [0.624]](http://kaggle.com/julian3833/pytorch-shortformer-w-chunks-train-0-624) (82)\n",
      "* [ğŸ“– PyTorch- \"ShortFormer\" w/Chunks - Infer [0.624]](http://kaggle.com/julian3833/pytorch-shortformer-w-chunks-infer-0-624) (69)\n",
      "* [ğŸ“– Topic Modeling with LDA](http://kaggle.com/julian3833/topic-modeling-with-lda) (15)\n",
      "* [ğŸ“– Pytorch - ITPT - Intra-task pre-training](http://kaggle.com/julian3833/pytorch-itpt-intra-task-pre-training) (11)\n",
      "* [ğŸ“– W&B-tracked Shortformers experiments](http://kaggle.com/julian3833/w-b-tracked-shortformers-experiments) (5)\n",
      "\n",
      "## [H&M Personalized Fashion Recommendations](https://www.kaggle.com/c/h-and-m-personalized-fashion-recommendations)\n",
      "**Provide product recommendations based on previous purchases**\n",
      "\n",
      "* [H&M - Implicit ALS model [0.014]](http://kaggle.com/julian3833/h-m-implicit-als-model-0-014) (163)\n",
      "* [H&M - Collaborative Filtering: User-user](http://kaggle.com/julian3833/h-m-collaborative-filtering-user-user) (63)\n",
      "* [H&M - Content-based: 12 most popular items [0.007]](http://kaggle.com/julian3833/h-m-content-based-12-most-popular-items-0-007) (29)\n",
      "\n",
      "## [G-Research Crypto Forecasting](https://www.kaggle.com/c/g-research-crypto-forecasting)\n",
      "**Use your ML expertise to predict real crypto market data**\n",
      "\n",
      "* [ğŸª™ğŸ’² G-Research- Starter LGBM Pipeline](http://kaggle.com/julian3833/g-research-starter-lgbm-pipeline) (266)\n",
      "* [ğŸª™ğŸ’² G-Research- Using the overlap fully [LB=0.99]](http://kaggle.com/julian3833/g-research-using-the-overlap-fully-lb-0-99) (161)\n",
      "* [ğŸª™ğŸ’² Proposal for a meaningful LB + Strict LGBM](http://kaggle.com/julian3833/proposal-for-a-meaningful-lb-strict-lgbm) (120)\n",
      "* [[S]ğŸª™ğŸ’²G-Research - Strict LGBM example [LB=0.017]](http://kaggle.com/julian3833/s-g-research-strict-lgbm-example-lb-0-017) (21)\n",
      "\n",
      "## [BirdCLEF 2022](https://www.kaggle.com/c/birdclef-2022)\n",
      "**Identify bird calls in soundscapes**\n",
      "\n",
      "* [ğŸ¦ Audio 101. 2- Detailed EDA](http://kaggle.com/julian3833/audio-101-2-detailed-eda) (8)\n",
      "* [ğŸ¦Audio 101. 1- Audio manipulation & musical notes](http://kaggle.com/julian3833/audio-101-1-audio-manipulation-musical-notes) (8)\n",
      "\n",
      "## [Other competitions]()\n",
      "****\n",
      "\n",
      "* [1 - Quick start: read csv and flatten json fields](http://kaggle.com/julian3833/1-quick-start-read-csv-and-flatten-json-fields) (491)\n",
      "* [1- Learning ğŸ¤— - Out-of-the-box BERT [LB: 0.577]](http://kaggle.com/julian3833/1-learning-out-of-the-box-bert-lb-0-577) (11)\n",
      "* [1- Learning ğŸ¤— - Out-of-the-box BERT [LB: 0.8102]](http://kaggle.com/julian3833/1-learning-out-of-the-box-bert-lb-0-8102) (14)\n",
      "* [2 - Quick study: LGBM, XGB and Catboost [LB: 1.66]](http://kaggle.com/julian3833/2-quick-study-lgbm-xgb-and-catboost-lb-1-66) (96)\n",
      "* [2. Learning ğŸ¤— - Out-of-the-box RoBERTa [LB: 0.53]](http://kaggle.com/julian3833/2-learning-out-of-the-box-roberta-lb-0-53) (18)\n",
      "* [3- Learning ğŸ¤— - Out-of-the-box Electra [LB: 0.58]](http://kaggle.com/julian3833/3-learning-out-of-the-box-electra-lb-0-58) (7)\n",
      "* [GPT-2 Large â€“774Mâ€“ w/Pytorch: Not that impressive](http://kaggle.com/julian3833/gpt-2-large-774m-w-pytorch-not-that-impressive) (14)\n",
      "* [Index of my public notebooks](http://kaggle.com/julian3833/index-of-my-public-notebooks) (1)\n",
      "* [ğŸš¢ 1 - Loading and visualizing the images](http://kaggle.com/julian3833/1-loading-and-visualizing-the-images) (33)\n",
      "* [ğŸš¢ 2 - Understanding & plotting rle bounding boxes](http://kaggle.com/julian3833/2-understanding-plotting-rle-bounding-boxes) (61)\n",
      "* [ğŸš¢ 3 - Basic exploratory analysis](http://kaggle.com/julian3833/3-basic-exploratory-analysis) (30)\n",
      "* [ğŸš¢ 4 - Exploring public models](http://kaggle.com/julian3833/4-exploring-public-models) (44)\n",
      "* [ğŸš¢ 5 - Submitting the test file (1.0 public LB)](http://kaggle.com/julian3833/5-submitting-the-test-file-1-0-public-lb) (35)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "txt_by_competition = by_competition()\n",
    "txt_hightlights = highlights(df)\n",
    "txt = txt_hightlights + \"\\n---\\n\" + txt_by_competition\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Highlights\n",
    "\n",
    "## [ğŸ¦  Sartorius - Starter Torch Mask R-CNN [LB=0.273]](http://kaggle.com/julian3833/sartorius-starter-torch-mask-r-cnn-lb-0-273) (278)\n",
    "A self-contained, simple, pure Torch Mask R-CNN implementation baseline model for the competition [Sartorius - Cell Instance Segmentation](https://www.kaggle.com/c/sartorius-cell-instance-segmentation).\n",
    "    This notebook has more than 600 forks and was a high-performing end-to-end simple solution for the problem at the very beginning of the competition.\n",
    "\n",
    "## [â˜£ï¸ Jigsaw - Incredibly Simple Naive Bayes [0.768]](http://kaggle.com/julian3833/jigsaw-incredibly-simple-naive-bayes-0-768) (186)\n",
    "A very simple naive bayes model with a high accuracy. \n",
    "    The condensed code has less than 20 lines and a slight -but smart- modification of it (adapted from a notebook by Jeremy Howard) would have landed a Bronze medal: [â˜£ï¸ Jigsaw - Jeremy Howard's NB [0.79744 Private]](https://www.kaggle.com/julian3833/jigsaw-jeremy-howard-s-nb-0-79744-private).\n",
    "\n",
    "## [ğŸ“– PyTorch- \"ShortFormer\" w/Chunks - Train [0.624]](http://kaggle.com/julian3833/pytorch-shortformer-w-chunks-train-0-624) (82)\n",
    "A baseline model for the competition [Feedback Prize - Evaluating Student Writing](https://www.kaggle.com/c/feedback-prize-2021). \n",
    "    It approaches the problems as a token classification problem (\"NER\"-like) and builds a RoBERTa base model with `max_length=512`. \n",
    "    In order to do so, it manages the chunking with stride of the texts with length greater than 512 (and the posterior merge). \n",
    "    A discussion about Longformers is presented as well.\n",
    "\n",
    "## [H&M - Implicit ALS model [0.014]](http://kaggle.com/julian3833/h-m-implicit-als-model-0-014) (163)\n",
    "Implicit ALS base model for the competition [H&M Personalized Fashion Recommendations](https://www.kaggle.com/c/h-and-m-personalized-fashion-recommendations).\n",
    "\n",
    "## [1 - Quick start: read csv and flatten json fields](http://kaggle.com/julian3833/1-quick-start-read-csv-and-flatten-json-fields) (491)\n",
    "A simple utility function that flattens json fields in a CSV when loaded with `pandas` for the competition [Google Analytics Customer Revenue Prediction](https://www.kaggle.com/c/ga-customer-revenue-prediction). \n",
    "    This problem appeared quite fast, so everyone was getting blocked right after loading the CSV. \n",
    "    That is why the notebook has almost 500 upvotes and more than 550 forks having just 10 lines of code. The competition had a terrible leak.\n",
    "\n",
    "## [GPT-2 Large â€“774Mâ€“ w/Pytorch: Not that impressive](http://kaggle.com/julian3833/gpt-2-large-774m-w-pytorch-not-that-impressive) (14)\n",
    "In this old notebook I applied the out-of-the-box GPT-2 models (`gpt2`, `gpt2-medium`, and `gpt2-large`) on the samples in the original blog post ([Better Language Models and Their Implications](https://openai.com/blog/better-language-models/)) using `huggingface`'s [pytorch-transformers](https://github.com/huggingface/pytorch-transformers) library, with a pretty simple code based on the library's [Quick Start](https://huggingface.co/pytorch-transformers/quickstart.html). \n",
    "The conclussion was that the results were good but not that impressive as the blogpost suggested.\n",
    "\n",
    "\n",
    "---\n",
    "# Organized by competition\n",
    "\n",
    "## [Sartorius - Cell Instance Segmentation](https://www.kaggle.com/c/sartorius-cell-instance-segmentation)\n",
    "**Detect single neuronal cells in microscopy images**\n",
    "\n",
    "* [ğŸ¦  Sartorius - Starter Torch Mask R-CNN [LB=0.273]](http://kaggle.com/julian3833/sartorius-starter-torch-mask-r-cnn-lb-0-273) (278)\n",
    "* [ğŸ¦  Sartorius - Starter Baseline Torch U-net [>0.0]](http://kaggle.com/julian3833/sartorius-starter-baseline-torch-u-net-0-0) (174)\n",
    "* [ğŸ¦  Sartorius - Classifier + Mask R-CNN [LB=0.28]](http://kaggle.com/julian3833/sartorius-classifier-mask-r-cnn-lb-0-28) (115)\n",
    "* [ğŸ¦  Sartorius - Resnet34 Classifier](http://kaggle.com/julian3833/sartorius-resnet34-classifier) (43)\n",
    "* [ğŸ¦  Sartorius - 3-line overlap-removing function](http://kaggle.com/julian3833/sartorius-3-line-overlap-removing-function) (13)\n",
    "\n",
    "## [chaii - Hindi and Tamil Question Answering](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering)\n",
    "**Identify the answer to questions found in Indian language passages**\n",
    "\n",
    "* [1 - The competition [QA for QA noobs ğŸ‡®ğŸ‡³]](http://kaggle.com/julian3833/1-the-competition-qa-for-qa-noobs) (36)\n",
    "* [2 - The dataset [QA for QA noobs ğŸ‡®ğŸ‡³]](http://kaggle.com/julian3833/2-the-dataset-qa-for-qa-noobs) (20)\n",
    "* [3 - The metric (Jaccard) [QA for QA noobs ğŸ‡®ğŸ‡³]](http://kaggle.com/julian3833/3-the-metric-jaccard-qa-for-qa-noobs) (19)\n",
    "* [4 - Exploring Public Models [QA for QA noobs ğŸ‡®ğŸ‡³]](http://kaggle.com/julian3833/4-exploring-public-models-qa-for-qa-noobs) (30)\n",
    "* [5-ğŸ¥‡XLM-Roberta+Torch's extra data [LB:0.749] ğŸ‡®ğŸ‡³](http://kaggle.com/julian3833/5-xlm-roberta-torch-s-extra-data-lb-0-749) (60)\n",
    "* [6- ğŸ¤— Pre & post-processing [QA for QA noobs ğŸ‡®ğŸ‡³]](http://kaggle.com/julian3833/6-pre-post-processing-qa-for-qa-noobs) (29)\n",
    "* [7 - Public Models Revisited [QA for QA noobs ğŸ‡®ğŸ‡³]](http://kaggle.com/julian3833/7-public-models-revisited-qa-for-qa-noobs) (30)\n",
    "* [Quick and Dirty Transliteration Tables ğŸ‡®ğŸ‡³](http://kaggle.com/julian3833/quick-and-dirty-transliteration-tables) (13)\n",
    "\n",
    "## [Jigsaw Rate Severity of Toxic Comments](https://www.kaggle.com/c/jigsaw-toxic-severity-rating)\n",
    "**Rank relative ratings of toxicity between comments**\n",
    "\n",
    "* [â˜£ï¸ Jigsaw - Incredibly Simple Naive Bayes [0.768]](http://kaggle.com/julian3833/jigsaw-incredibly-simple-naive-bayes-0-768) (186)\n",
    "* [â˜£ï¸ Jigsaw - Early Ensemble [LB=0.836]](http://kaggle.com/julian3833/jigsaw-early-ensemble-lb-0-836) (103)\n",
    "* [â˜£ï¸ Jigsaw - ğŸ¤— HF hub out-of-the-box models](http://kaggle.com/julian3833/jigsaw-hf-hub-out-of-the-box-models) (31)\n",
    "* [â˜£ï¸ Jigsaw - Explore Previous Competitions Datasets](http://kaggle.com/julian3833/jigsaw-explore-previous-competitions-datasets) (9)\n",
    "* [â˜£ï¸ Jigsaw - Random rank [LB=0.498]](http://kaggle.com/julian3833/jigsaw-random-rank-lb-0-498) (9)\n",
    "* [â˜£ï¸ Jigsaw - Jeremy Howard's NB [0.79744 Private]](http://kaggle.com/julian3833/jigsaw-jeremy-howard-s-nb-0-79744-private) (7)\n",
    "* [â˜£ï¸ Jigsaw - New Ensemble [LB=0.853]](http://kaggle.com/julian3833/jigsaw-new-ensemble-lb-0-853) (1)\n",
    "\n",
    "## [TensorFlow - Help Protect the Great Barrier Reef](https://www.kaggle.com/c/tensorflow-great-barrier-reef)\n",
    "**Detect crown-of-thorns starfish in underwater image data**\n",
    "\n",
    "* [ğŸ  Reef - A CV strategy: subsequences!](http://kaggle.com/julian3833/reef-a-cv-strategy-subsequences) (221)\n",
    "* [ğŸ  Reef- Starter Torch FasterRCNN Train [LB=0.416]](http://kaggle.com/julian3833/reef-starter-torch-fasterrcnn-train-lb-0-416) (136)\n",
    "* [ğŸ  Reef- Starter Torch FasterRCNN Infer [LB=0.416]](http://kaggle.com/julian3833/reef-starter-torch-fasterrcnn-infer-lb-0-416) (90)\n",
    "* [ğŸ  DETR - Detection Transformer - Train [0.189]](http://kaggle.com/julian3833/detr-detection-transformer-train-0-189) (32)\n",
    "* [ğŸ  DETR - Detection Transformer - Infer [0.189]](http://kaggle.com/julian3833/detr-detection-transformer-infer-0-189) (27)\n",
    "* [ğŸ  Reef - Minimal EDA](http://kaggle.com/julian3833/reef-minimal-eda) (4)\n",
    "\n",
    "## [Feedback Prize - Evaluating Student Writing](https://www.kaggle.com/c/feedback-prize-2021)\n",
    "**Analyze argumentative writing elements from students grade 6-12**\n",
    "\n",
    "* [ğŸ“–Feedback- BaselineğŸ¤— Sentence Classifier [0.226]](http://kaggle.com/julian3833/feedback-baseline-sentence-classifier-0-226) (189)\n",
    "* [ğŸ“– PyTorch- \"ShortFormer\" w/Chunks - Train [0.624]](http://kaggle.com/julian3833/pytorch-shortformer-w-chunks-train-0-624) (82)\n",
    "* [ğŸ“– PyTorch- \"ShortFormer\" w/Chunks - Infer [0.624]](http://kaggle.com/julian3833/pytorch-shortformer-w-chunks-infer-0-624) (69)\n",
    "* [ğŸ“– Topic Modeling with LDA](http://kaggle.com/julian3833/topic-modeling-with-lda) (15)\n",
    "* [ğŸ“– Pytorch - ITPT - Intra-task pre-training](http://kaggle.com/julian3833/pytorch-itpt-intra-task-pre-training) (11)\n",
    "* [ğŸ“– W&B-tracked Shortformers experiments](http://kaggle.com/julian3833/w-b-tracked-shortformers-experiments) (5)\n",
    "\n",
    "## [H&M Personalized Fashion Recommendations](https://www.kaggle.com/c/h-and-m-personalized-fashion-recommendations)\n",
    "**Provide product recommendations based on previous purchases**\n",
    "\n",
    "* [H&M - Implicit ALS model [0.014]](http://kaggle.com/julian3833/h-m-implicit-als-model-0-014) (163)\n",
    "* [H&M - Collaborative Filtering: User-user](http://kaggle.com/julian3833/h-m-collaborative-filtering-user-user) (63)\n",
    "* [H&M - Content-based: 12 most popular items [0.007]](http://kaggle.com/julian3833/h-m-content-based-12-most-popular-items-0-007) (29)\n",
    "\n",
    "## [G-Research Crypto Forecasting](https://www.kaggle.com/c/g-research-crypto-forecasting)\n",
    "**Use your ML expertise to predict real crypto market data**\n",
    "\n",
    "* [ğŸª™ğŸ’² G-Research- Starter LGBM Pipeline](http://kaggle.com/julian3833/g-research-starter-lgbm-pipeline) (266)\n",
    "* [ğŸª™ğŸ’² G-Research- Using the overlap fully [LB=0.99]](http://kaggle.com/julian3833/g-research-using-the-overlap-fully-lb-0-99) (161)\n",
    "* [ğŸª™ğŸ’² Proposal for a meaningful LB + Strict LGBM](http://kaggle.com/julian3833/proposal-for-a-meaningful-lb-strict-lgbm) (120)\n",
    "* [[S]ğŸª™ğŸ’²G-Research - Strict LGBM example [LB=0.017]](http://kaggle.com/julian3833/s-g-research-strict-lgbm-example-lb-0-017) (21)\n",
    "\n",
    "## [BirdCLEF 2022](https://www.kaggle.com/c/birdclef-2022)\n",
    "**Identify bird calls in soundscapes**\n",
    "\n",
    "* [ğŸ¦ Audio 101. 2- Detailed EDA](http://kaggle.com/julian3833/audio-101-2-detailed-eda) (8)\n",
    "* [ğŸ¦Audio 101. 1- Audio manipulation & musical notes](http://kaggle.com/julian3833/audio-101-1-audio-manipulation-musical-notes) (8)\n",
    "\n",
    "## [Other competitions]()\n",
    "****\n",
    "\n",
    "* [1 - Quick start: read csv and flatten json fields](http://kaggle.com/julian3833/1-quick-start-read-csv-and-flatten-json-fields) (491)\n",
    "* [1- Learning ğŸ¤— - Out-of-the-box BERT [LB: 0.577]](http://kaggle.com/julian3833/1-learning-out-of-the-box-bert-lb-0-577) (11)\n",
    "* [1- Learning ğŸ¤— - Out-of-the-box BERT [LB: 0.8102]](http://kaggle.com/julian3833/1-learning-out-of-the-box-bert-lb-0-8102) (14)\n",
    "* [2 - Quick study: LGBM, XGB and Catboost [LB: 1.66]](http://kaggle.com/julian3833/2-quick-study-lgbm-xgb-and-catboost-lb-1-66) (96)\n",
    "* [2. Learning ğŸ¤— - Out-of-the-box RoBERTa [LB: 0.53]](http://kaggle.com/julian3833/2-learning-out-of-the-box-roberta-lb-0-53) (18)\n",
    "* [3- Learning ğŸ¤— - Out-of-the-box Electra [LB: 0.58]](http://kaggle.com/julian3833/3-learning-out-of-the-box-electra-lb-0-58) (7)\n",
    "* [GPT-2 Large â€“774Mâ€“ w/Pytorch: Not that impressive](http://kaggle.com/julian3833/gpt-2-large-774m-w-pytorch-not-that-impressive) (14)\n",
    "* [Index of my public notebooks](http://kaggle.com/julian3833/index-of-my-public-notebooks) (1)\n",
    "* [ğŸš¢ 1 - Loading and visualizing the images](http://kaggle.com/julian3833/1-loading-and-visualizing-the-images) (33)\n",
    "* [ğŸš¢ 2 - Understanding & plotting rle bounding boxes](http://kaggle.com/julian3833/2-understanding-plotting-rle-bounding-boxes) (61)\n",
    "* [ğŸš¢ 3 - Basic exploratory analysis](http://kaggle.com/julian3833/3-basic-exploratory-analysis) (30)\n",
    "* [ğŸš¢ 4 - Exploring public models](http://kaggle.com/julian3833/4-exploring-public-models) (44)\n",
    "* [ğŸš¢ 5 - Submitting the test file (1.0 public LB)](http://kaggle.com/julian3833/5-submitting-the-test-file-1-0-public-lb) (35)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
